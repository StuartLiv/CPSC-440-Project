{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StuartLiv/CPSC-440-Project/blob/main/models/hooks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms.functional as F"
      ],
      "metadata": {
        "id": "IRGbGgqMyPTE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from keras.preprocessing.image import img_to_array # TODO don't use keras\n",
        "from keras.utils import load_img\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device\", device)"
      ],
      "metadata": {
        "id": "N-_96eljZSnh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c19a427-1b70-493a-e552-c0d4fa6e523c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Set up project in runtime\n",
        "Copy the following cell and run it with your credentials to clone the repo, getting datasets as runtime files, and then delete your credentials."
      ],
      "metadata": {
        "id": "p2hPm3ge3aAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email {userdata.get('email')}\n",
        "!git config --global user.name {userdata.get('name')}\n",
        "!git clone https://{userdata.get('token')}@github.com/StuartLiv/CPSC-440-Project\n",
        "%pwd"
      ],
      "metadata": {
        "id": "tnRzeKSA1q4F",
        "outputId": "68c6d673-40fe-48dc-d728-b7d922d2d158",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'CPSC-440-Project' already exists and is not an empty directory.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preprocess data\n",
        "\n",
        "Datasets:\n",
        "- `medset_multisize`\n",
        "  - Single datasets. Dataset construction code in repo too\n",
        "  - 12000 Images, in randomized order\n",
        "  - Res 256x256 (grayscale images also available in 128x128, 64x64, 32x32)\n"
      ],
      "metadata": {
        "id": "y1rkmEoxY0bM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Returns an array containing all images in folder, sorted by filename. Useful for color/gray/downsize versions in different folders with same filename.\n",
        "@param path - directory with images\n",
        "@return\n",
        "'''\n",
        "def make_image_arr(path):\n",
        "\n",
        "  def sorted_alphanumeric(data):\n",
        "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
        "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)',key)]\n",
        "    return sorted(data,key = alphanum_key)\n",
        "\n",
        "  files = os.listdir(path)\n",
        "  files = sorted_alphanumeric(files)\n",
        "\n",
        "  arr = []\n",
        "  for i in tqdm(files):\n",
        "    img = load_img(path + '/'+i)\n",
        "    arr.append(img_to_array(img) / 255)\n",
        "\n",
        "  return np.array(arr)\n"
      ],
      "metadata": {
        "id": "nmb6y7FQUO24"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load datasets, partition sets, and make tensors in color-layer order for convolutions\n",
        "\n"
      ],
      "metadata": {
        "id": "klqfJGcYZnmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_proportion = 0.8\n",
        "validation_proportion = 0.0\n",
        "\n",
        "def load_and_partition(path):\n",
        "  arr = make_image_arr(path)\n",
        "  cutoff = int(len(arr)*training_proportion)\n",
        "  return arr[:cutoff], arr[cutoff:]\n",
        "\n",
        "def make_tensor(arr):\n",
        "  #Axis transformation: Shape (n, h, w, 3) -> (n, 3, h, w)\n",
        "  #Inversion is np.moveaxis(arr, [2,2], [1,1])\n",
        "  return torch.from_numpy(np.moveaxis(arr, [3,1], [1,2]))\n",
        "\n",
        "#Extract single channel, since all 3 are the same in gray images\n",
        "def make_gray_tensor(arr):\n",
        "  return make_tensor(arr)[:, 0:1, :, :]\n",
        "\n",
        "gray_train, gray_test = load_and_partition('/content/CPSC-440-Project/datasets/medset_multisize/gray32')\n",
        "\n",
        "gray_train_tensors = make_gray_tensor(gray_train)\n",
        "gray_test_tensors = make_gray_tensor(gray_test)"
      ],
      "metadata": {
        "id": "nbS2bT3vXKJc",
        "outputId": "05210a1f-96d0-46f1-8675-812d0fd22668",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12000/12000 [00:02<00:00, 5753.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model\n",
        "Model generated below:"
      ],
      "metadata": {
        "id": "WnA0Wvam63vM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kL2B0zg4cEoR"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 256, 3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 16, 3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(8, 3, 3, padding=1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "# Create the autoencoder\n",
        "autoencoder = Autoencoder()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Model (SGD)"
      ],
      "metadata": {
        "id": "7bE0wD5tOKG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.encoder = torch.load('/content/CPSC-440-Project/models/32_dim_encoder_weights')\n",
        "autoencoder.decoder = torch.load('/content/CPSC-440-Project/models/decoder_weights_256')\n",
        "autoencoder.eval()\n"
      ],
      "metadata": {
        "id": "NlEig1rTOP2F",
        "outputId": "1c0ae182-329e-4d02-b029-a0c0ce74dad8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Autoencoder(\n",
              "  (encoder): Sequential(\n",
              "    (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (decoder): Sequential(\n",
              "    (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "    (6): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): ReLU()\n",
              "    (9): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (10): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): ReLU()\n",
              "    (12): ConvTranspose2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "    (13): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (14): ReLU()\n",
              "    (15): Conv2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (16): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hook\n",
        "Add hooks to self.encoder or self.decoder, with the indices listed by `.eval` above!"
      ],
      "metadata": {
        "id": "DR6WIpr42v2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# a dict to store the activations\n",
        "activation = {}\n",
        "def getActivation(name):\n",
        "  # the hook signature\n",
        "  def hook(model, input, output):\n",
        "    activation[name] = output.detach().cpu().numpy()\n",
        "  return hook\n",
        "\n",
        "# register forward hooks on the layers with the following dims:\n",
        "dims = [32,64,128,256]\n",
        "autoencoder.encoder[-1].register_forward_hook(getActivation('en'))\n",
        "autoencoder.decoder[0].register_forward_hook(getActivation('de64'))\n",
        "autoencoder.decoder[6].register_forward_hook(getActivation('de128'))\n",
        "autoencoder.decoder[12].register_forward_hook(getActivation('de256'))\n",
        "\n",
        "# h3 = autoencoder.decoder.register_forward_hook(getActivation('sigmoid')) # this is excatly the output!\n"
      ],
      "metadata": {
        "id": "Key7WnQau7CU",
        "outputId": "a2e0303f-ae8e-4545-f27d-f7f5a1bbafe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.hooks.RemovableHandle at 0x7e11451265c0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rank(S,cut):\n",
        "  sum = np.sum(S)\n",
        "  i = 0\n",
        "  while np.sum(S[:i]) < cut * np.sum(S[i:]):\n",
        "    i += 1\n",
        "  return i"
      ],
      "metadata": {
        "id": "lNQXikNVM4xW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 100\n",
        "ranks = np.zeros((3,n))\n",
        "count = 0\n",
        "for i in tqdm(np.random.randint(0, len(gray_test), n)):\n",
        "  # Colorize the image\n",
        "  t = gray_test_tensors[i].unsqueeze(0).to(device)\n",
        "  autoencoder(t)\n",
        "\n",
        "  for j in range(3):\n",
        "    key = list(activation.keys())[j]\n",
        "    dim = dims[j]\n",
        "    print(activation[key][0].shape)\n",
        "    U,S,V = np.linalg.svd(activation[key][0].reshape(-1,dim**2))\n",
        "    print(S.shape)\n",
        "    ranks[j,count] = get_rank(S,3)\n",
        "\n",
        "  count += 1"
      ],
      "metadata": {
        "id": "sHvbUZu1MWfW",
        "outputId": "20ecefb9-8e7e-4e86-b4f0-32460c483680",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256, 32, 32)\n",
            "(256,)\n",
            "(128, 64, 64)\n",
            "(128,)\n",
            "(32, 128, 128)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 1/100 [00:09<15:59,  9.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,)\n",
            "(256, 32, 32)\n",
            "(256,)\n",
            "(128, 64, 64)\n",
            "(128,)\n",
            "(32, 128, 128)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 2/100 [00:19<16:00,  9.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,)\n",
            "(256, 32, 32)\n",
            "(256,)\n",
            "(128, 64, 64)\n",
            "(128,)\n",
            "(32, 128, 128)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 3/100 [00:28<15:26,  9.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,)\n",
            "(256, 32, 32)\n",
            "(256,)\n",
            "(128, 64, 64)\n",
            "(128,)\n",
            "(32, 128, 128)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 4/100 [00:38<15:08,  9.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,)\n",
            "(256, 32, 32)\n",
            "(256,)\n",
            "(128, 64, 64)\n",
            "(128,)\n",
            "(32, 128, 128)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 5/100 [00:47<15:02,  9.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,)\n",
            "(256, 32, 32)\n",
            "(256,)\n",
            "(128, 64, 64)\n",
            "(128,)\n",
            "(32, 128, 128)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 6/100 [00:57<14:55,  9.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,)\n",
            "(256, 32, 32)\n",
            "(256,)\n",
            "(128, 64, 64)\n",
            "(128,)\n",
            "(32, 128, 128)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 7/100 [01:06<14:42,  9.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,)\n",
            "(256, 32, 32)\n",
            "(256,)\n",
            "(128, 64, 64)\n",
            "(128,)\n",
            "(32, 128, 128)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 8/100 [01:16<14:29,  9.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,)\n",
            "(256, 32, 32)\n",
            "(256,)\n",
            "(128, 64, 64)\n",
            "(128,)\n",
            "(32, 128, 128)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 9/100 [01:25<14:20,  9.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,)\n",
            "(256, 32, 32)\n",
            "(256,)\n",
            "(128, 64, 64)\n",
            "(128,)\n",
            "(32, 128, 128)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 10/100 [01:35<14:21,  9.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,)\n",
            "(256, 32, 32)\n",
            "(256,)\n",
            "(128, 64, 64)\n",
            "(128,)\n",
            "(32, 128, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ranks"
      ],
      "metadata": {
        "id": "ga5JtS8b5h-M",
        "outputId": "8d788676-490f-4892-9d2e-89037da09ee4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 6.],\n",
              "       [ 5.],\n",
              "       [10.]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.linalg.svd(activation['de256'][0].reshape(8,256**2)[:4])"
      ],
      "metadata": {
        "id": "FaQRs_G81M2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rank of convolutions\n",
        "# out of max 16, 64, 256\n",
        "np.mean(ranks, axis=1)"
      ],
      "metadata": {
        "id": "TEh38dHvX1-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('ranks.npy',ranks)"
      ],
      "metadata": {
        "id": "4tGhv7__I6Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Push changes\n",
        "\n",
        "Look for any changes in the project directory, excluding this notebook, and push them.\n"
      ],
      "metadata": {
        "id": "UuwWuVpI7JOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd CPSC-440-Project"
      ],
      "metadata": {
        "id": "hxef_XMPdOLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Save notebook\n",
        "\n",
        "Commiting this notebook requires a special maneuver:\n",
        "\n",
        "> File > Save a copy in github > enter original `path` + new commit msg\n",
        "\n",
        "That's it!"
      ],
      "metadata": {
        "id": "l6r4vK-a9Tr4"
      }
    }
  ]
}